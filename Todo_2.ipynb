{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "**TODO 1**: Lorem Ipsum is just a random txt that devs use as a placeholder for multiple things (especially web developping) when you don't have the real text and just want to test your functionnality. Put a [Lorem Ipsum](https://www.lipsum.com/) of 3 paragraphs in a txt file using python, each paragraph delimited by two new line.\n",
    "\n",
    "**TODO 2**: Update the txt file by removing the first paragraph.\n",
    "\n",
    "**TODO 3**: Create a dict from the paper of [lecun et al.](https://www.researchgate.net/publication/277411157_Deep_Learning) and [goodfellow et al.](https://arxiv.org/abs/1406.2661) with authors, title, affiliations.\n",
    "\n",
    "**TODO 4**: Save the previously created dict in the JSON format and load it back.\n",
    "\n",
    "**TODO 5**: Save the previously created dict in the pickle format. Try to open manually (i.e with a text editor), is it human readable ?\n",
    "\n",
    "**TODO 6**: Parse the xml_file2 in the same way as seen in the lecture: put infos in a dict and save it in a json file.\n",
    "\n",
    "**TODO 7**: Download an image of your choice and save it in either jpg or png.\n",
    "\n",
    "**TODO 8**: From the data/Chap2/data_world.json file, create a set of publisher type.\n",
    "\n",
    "**TODO 9**: From the data/Chap2/data_world.json file, delete the key of your choice and save the new dict as data_world_cleaned.json.\n",
    "\n",
    "**TODO 10 bonus**: From the data/Chap2/data_world.json file, create the co-occurence matrix between \"accessLevel\" and \"accrualPeriodicity\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##1\n",
    "\n",
    "import lorem\n",
    "\n",
    "with open('lorem_ipsum.txt', 'w') as file:\n",
    "    for i in range(3):\n",
    "        paragraph = lorem.paragraph()\n",
    "        file.write(paragraph)\n",
    "        file.write('\\n\\n')\n",
    "##2\n",
    "\n",
    "with open('lorem_ipsum.txt', 'r') as file:\n",
    "    paragraphs = file.readlines()\n",
    "\n",
    "paragraphs.pop(0)\n",
    "\n",
    "with open('lorem_ipsum.txt', 'w') as file:\n",
    "    for paragraph in paragraphs:\n",
    "        file.write(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##3\n",
    "\n",
    "papers = {\n",
    "    \"lecun\": {\n",
    "        \"authors\": [\"Yann LeCun\", \"Yoshua Bengio\", \"Geoffrey Hinton\"],\n",
    "        \"title\": \"Deep Learning\",\n",
    "        \"affiliations\": [\"New York University\", \"University of Montreal\", \"University of Toronto\"]\n",
    "    },\n",
    "    \"goodfellow\": {\n",
    "        \"authors\": [\"Ian J. Goodfellow\", \"Yoshua Bengio\", \"Aaron Courville\"],\n",
    "        \"title\": \"Deep Learning\",\n",
    "        \"affiliations\": [\"Google\", \"University of Montreal\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "##4\n",
    "\n",
    "import json\n",
    "with open('papers.json', 'w') as file:\n",
    "    json.dump(papers, file)\n",
    "with open('papers.json', 'r') as file:\n",
    "    loaded_papers = json.load(file)\n",
    "print(loaded_papers)\n",
    "\n",
    "##5\n",
    "\n",
    "import pickle\n",
    "with open('papers.pickle', 'wb') as file:\n",
    "    pickle.dump(papers, file)\n",
    "with open('papers.pickle', 'rb') as file:\n",
    "    contents = file.read()\n",
    "\n",
    "print(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##6\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "\n",
    "tree = ET.parse('C:/Users/epcmic/OneDrive/Bureau/NoSQL/NoSQL/data/Chap2/xml_file2.nxml')\n",
    "root = tree.getroot()\n",
    "\n",
    "data = {}\n",
    "for child in root:\n",
    "    if child.tag == 'name':\n",
    "        data['name'] = child.text\n",
    "    elif child.tag == 'age':\n",
    "        data['age'] = int(child.text)\n",
    "    elif child.tag == 'email':\n",
    "        data['email'] = child.text\n",
    "    elif child.tag == 'phone':\n",
    "        data['phone'] = child.text\n",
    "\n",
    "with open('xml_data.json', 'w') as file:\n",
    "    json.dump(data, file)\n",
    "\n",
    "with open('xml_data.json', 'r') as file:\n",
    "    contents = json.load(file)\n",
    "    print(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##7\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "url = \"https://example.com/image.jpg\"\n",
    "response = requests.get(url)\n",
    "\n",
    "with open(\"minion.jpg\", \"wb\") as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "with open(\"minion.jpg\", \"wb\") as file:\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "public | irregular | 5521\n",
      "public | R/P1D | 8\n",
      "public | R/P1M | 4\n",
      "public | Unknown | 29\n",
      "public | R/PT1S | 1\n",
      "public | R/P3M | 1\n"
     ]
    }
   ],
   "source": [
    "##8\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"C:/Users/epcmic/OneDrive/Documents/GitHub/NoSQL/data/Chap2/data_world.json\", 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "publisher_types = set()\n",
    "for book in data:\n",
    "    if 'publisher_type' in book:\n",
    "        publisher_types.add(book['publisher_type'])\n",
    "\n",
    "print(publisher_types)\n",
    "\n",
    "##9\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "with open('C:/Users/epcmic/OneDrive/Documents/GitHub/NoSQL/data/Chap2/data_world.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for book in data:\n",
    "    if 'subtitle' in book:\n",
    "        del book['subtitle']\n",
    "\n",
    "if not os.path.exists('data/Chap2'):\n",
    "    os.makedirs('data/Chap2')\n",
    "with open('data/Chap2/data_world_cleaned.json', 'w') as file:\n",
    "    json.dump(data, file)\n",
    "\n",
    "\n",
    "##10\n",
    "\n",
    "with open('C:/Users/epcmic/OneDrive/Documents/GitHub/NoSQL/data/Chap2/data_world.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for book in data:\n",
    "    access_level = book['accessLevel']\n",
    "    if 'accrualPeriodicity' in book:\n",
    "        accrual_periodicity = book['accrualPeriodicity']\n",
    "    else:\n",
    "        accrual_periodicity = 'Unknown'\n",
    "    if access_level not in matrix:\n",
    "        matrix[access_level] = {}\n",
    "    if accrual_periodicity not in matrix[access_level]:\n",
    "        matrix[access_level][accrual_periodicity] = 0\n",
    "    matrix[access_level][accrual_periodicity] += 1\n",
    "\n",
    "for access_level in matrix:\n",
    "    for accrual_periodicity in matrix[access_level]:\n",
    "        count = matrix[access_level][accrual_periodicity]\n",
    "        print(f'{access_level} | {accrual_periodicity} | {count}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
